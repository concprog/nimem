# Memory Systems

Usually, memory systems have three very distinct features to them that separate them from the basic semantic search and RAG systems.
- Episodic segmentation of context, which depends on the topic, or subject of the discussion usually. The LLM extracts these from the 
- Transforming user-sent context into triplets (triplet extraction) for graph memory
  - this involves replacements(updates) to the graph memory as well, which might be impossible to do accurately without an LLM
- Entity and key data storage
- (Optional, but widely seen) Tool prompts, allowing the memory to be used as another tool by an agent or allowing internal parameters to set by an agent

## Differentiation: Why Nimem?
GLiNER2 and FastCoref provide the necessary tooling to extract and link structured data from text without the need for expensive fine-tuning. We attempt to improve over Graphiti as the baseline memory retrieval system by **shifting the burden of structure from slow, probabilistic LLM calls to millisecond-latency deterministic models, and by enriching the rigid knowledge graph with unsupervised vector clustering to capture the implicit 'weak' relations that purely symbolic systems miss.**

## A Review of Memory
Here are some examples of **frameworks**, not just memory systems. I believe many memory systems are red herrings, in the sense that they do not provide a robust way to manage the different types of data that should be in memory, rather they put everything in a bin and separate them the best they can. Sounds familiar? Well, that's RAG, but with more sources and a structured prompt that you don't have to write.
Nice in theory, but when it fails there usually is a long session of looking at source code and prompts, and understanding the logic of the numerous steps taken to store data, and then promptly forget to retrieve it.

Google ADK separates the problem of two different forms of data stored by their agents, which are user data and session data. Ideally, each session should be stored separately and carry the context of agent behaviour and conversational memory, while user data is used to identify which sessions belong to the same user.  
If needed, the session data for the user can be stored and retrieved as needed.

Graphiti provides a rather interesting take on Graph Memory, it uses prompts to extract, update and delete relations in order to always keep them updated. It also is a fusion of graph and episodic memory of sorts, with episodes being temporal and Communities, or rather clusters of graph nodes representing similar topics.

## Without Intelligence  
Intelligence causes many of the features of the modern day memory system. Triplet extraction, Episodic segmentation, Even storing entities and key values.
I define non-intelligent systems here as those who do not use LLMs as part of the decision process.
So this means embedding models, coreference and NLP techniques are fair game.

**Note**: Clustering and finding centroids (using hdbscan/fast_hdbscan/another fast and good clustering method) is a good way to find centroids of topics that you can then arrange in communitites/colllections such as in Graphiti.

**Another Note:** Using coreference after finding entities and substituting them sentencewise is a good way to find facts about entities. Then you can find similar sentences using rank_bm25 and time them so you have valid and invalid timestamps a la Graphiti

## Libraries
- **Entity & Relation Extraction**: [GLiNER2](https://github.com/urchade/GLiNER) - Schema-driven extraction.
- **Coreference Resolution**: [FastCoref](https://github.com/shon-otmazgin/fastcoref) - Fast, neural resolution.
- **Embeddings**: [infinity-emb](https://github.com/michaelfeil/infinity) - High-performance embedding server/library.
- **Graph Storage**: [FalkorDBLite](https://github.com/FalkorDB/falkordb-lite) - Embedded, bitemporal graph database.
- **Clustering**: [FastHDBSCAN](https://github.com/Tcm0/fast_hdbscan) - Density-based clustering for topic discovery.

## Objective
To create a memory system that uses concepts from Zep/Graphiti such as episodic segmentation, fact invalidation (use tense etc to determine time) and entity relations, but without the need for LLMs.
Embeddings, entity extraction and coreference are all done without LLMs.

## System Overview
**Nimem** is a low-latency, privacy-focused episodic memory system designed to operate without generative LLMs in the ingestion loop. It leverages a Neuro-Symbolic architecture combining neural text processing with a bitemporal knowledge graph.

### Core Philosophy
- **Deterministic Ingestion**: Facts are extracted using specific schema definitions, not open-ended generation.
- **Privacy First**: All processing happens locally; no data leaves the environment.
- **Bitemporality**: The system knows *when* an event happened vs. *when* it learned about it, enabling "time travel" for agent state.

### Architecture
1.  **Ingestion**:
    *   **Coreference**: `FastCoref` resolves pronouns (He/She/It) to specific entities.
    *   **Extraction**: `GLiNER2` extracts structured triples (Subject, Relation, Object) and Entities based on a schema.
2.  **Storage**:
    *   **Graph**: `FalkorDBLite` stores the knowledge graph with bitemporal edges.
    *   **Vectors**: `Infinity-emb` generates embeddings for nodes to allow semantic search.
3.  **Synthesis**:
    *   **Clustering**: `FastHDBSCAN` groups related entities into topics (Weak Relations) to find implicit connections.

## Data Flow & Relation Types

### Information Pipeline
1.  **Raw Input**: User sends a message (e.g., "Alice and Bob are working on the Protocol.").
2.  **Normalization**: `FastCoref` rewrites pronouns to explicit names ("Alice and Bob are working on the Protocol.").
3.  **Strong Relation Extraction**:
    *   `GLiNER2` scans for defined relations in `schema.py` (e.g., `WORKS_ON`).
    *   **Result**: Explicit Edge `(Alice)-[WORKS_ON]->(Protocol)`, `(Bob)-[WORKS_ON]->(Protocol)`.
    *   These are **Strong Relations**: High confidence, explicitly stated in text.
4.  **Graph Ingestion**:
    *   Nodes/Edges are stored in `FalkorDBLite`.
    *   Embeddings for "Alice", "Bob", "Protocol" are generated via `infinity-emb` and stored.
5.  **Weak Relation Discovery (Async/Batch)**:
    *   `FastHDBSCAN` runs on node embeddings.
    *   It notices "Alice" and "Bob" appear in similar semantic contexts (vectors are close).
    *   **Result**: Topic Cluster identified (e.g., "Topic_1").
    *   System creates **Weak Relations**: `(Alice)-[BELONGS_TO]->(Topic_1)`, `(Bob)-[BELONGS_TO]->(Topic_1)`.
    *   This links Alice and Bob implicitly, even if no direct "Alice knows Bob" sentence exists.

### Relation Taxonomy
| Type | Source | Reliability | Purpose |
|------|--------|-------------|---------|
| **Strong** | Explicit Text (GLiNER) | High | Factual accuracy, direct query answering. |
| **Weak** | Vector Clustering (HDBSCAN) | Moderate | Discovery, thematic recall, associating disjoint facts. |

## Research
Technical Verification and Architectural Analysis: Non-LLM Episodic Memory System1. Executive SummaryThis report delivers a comprehensive technical verification and architectural critique of a proposed implementation plan for a Non-LLM Episodic Memory System. The analysis is predicated on a rigorous examination of the specified technology stack—Stanza, Infinity-emb, and Graphiti—against the requirements of modern agentic workflows, specifically focusing on latency, temporal reasoning, and independence from generative Large Language Model (LLM) bottlenecks during the memory formation process.The investigation confirms that the proposed shift from purely context-window-dependent memory (often pejoratively termed "Goldfish Memory" in recent literature 1) to a neuro-symbolic, graph-based architecture is technically sound and aligned with the cutting edge of enterprise AI development.2 The architectural premise of utilizing a bitemporal knowledge graph to handle the mutability of facts over time—distinguishing between when an event occurred (Valid Time) and when it was recorded (Transaction Time)—is validated as a critical capability absent in standard vector-based Retrieval Augmented Generation (RAG) systems.3However, the specific component selection within the implementation plan reveals significant inefficiencies that threaten the "Non-LLM" and "Low Latency" objectives. Most notably, the reliance on Stanza for Open Information Extraction (OpenIE) and Coreference Resolution introduces a prohibitive latency bottleneck due to its dependency on the Java-based CoreNLP server, creating unnecessary inter-process communication overhead.5 The analysis recommends replacing this component with GLiNER (Generalist Lightweight Neural Entity Recognition) and FastCoref, which offer order-of-magnitude speed improvements via distilled, Python-native architectures.7Furthermore, the report identifies a critical nuance in the categorization of Graphiti. While positioned as part of a "Non-LLM" system, Graphiti's ingestion pipeline is fundamentally dependent on LLMs for entity and relationship extraction.9 This creates a "hybrid" rather than "non-LLM" architecture. The verification of Infinity-emb confirms it as a robust choice, particularly with its support for ModernBERT and ONNX runtime acceleration, which ensures high-throughput semantic embedding.10Finally, an audit of the referenced benchmarks—LoCoMo, DMR (Deep Memory Retrieval), and EpBench—suggests that claims of "super-human" or "solved" long-term memory should be viewed with skepticism. While the Zep/Graphiti architecture achieves state-of-the-art results on the DMR benchmark (94.8% accuracy) 12, this metric primarily measures retrieval precision rather than the complex temporal reasoning required for true episodic intelligence, where even advanced models still lag significantly behind human baselines.132. Introduction: The Crisis of Context and the Memory ImperativeThe deployment of autonomous AI agents in long-running, multi-session environments has exposed the fundamental limitations of the Transformer architecture's reliance on the context window as a primary memory mechanism. This phenomenon, often described as the "Goldfish Memory" problem, dictates that as interactions proceed, earlier information is either truncated or diluted by the attention mechanism's focus on recent tokens.3 While context windows have expanded—with some models supporting up to 1 million tokens—the computational cost of processing such contexts scales quadratically (or linearly with optimizations), and the "Lost-in-the-Middle" phenomenon persists, degrading retrieval accuracy for information situated in the mid-range of the context.152.1 The Architectural Shift: From Context to StoreTo address these limitations, the industry is shifting toward "Infinite Memory" architectures that externalize the agent's history into structured, queryable formats. This transition moves away from the ephemeral "Context Window" toward persistent "Episodic Memory Systems." These systems must satisfy three core requirements:Permanence: Memories must persist beyond the session lifecycle and be immune to context flushing.Manipulability: The system must support the updating, invalidation, and forgetting of facts. Standard vector stores (semantic memory) struggle here, as "forgetting" a vector is mathematically non-trivial compared to deleting a graph edge.Temporal Grounding: Events must be anchored in time, allowing the agent to distinguish between current truths (e.g., "User lives in New York") and historical facts ("User lived in London").2.2 The Proposed Solution: Neuro-Symbolic IntegrationThe verified implementation plan proposes a Neuro-Symbolic architecture. It combines the semantic flexibility of neural networks (embeddings, language models) with the structural rigidity and logical precision of symbolic systems (Knowledge Graphs).Neural Layer: Handles the fuzziness of natural language, converting user utterances into mathematical vectors (via Infinity-emb) or extracting entities (via Stanza/GLiNER).Symbolic Layer: Stores these entities in a Knowledge Graph (Graphiti) that enforces schema constraints and temporal validity.This report will dissect each layer of this proposed stack, validating the library choices against the stated goals of performance ("Non-LLM" latency) and capability (Temporal Reasoning).3. Technical Verification: The Natural Language Processing LayerThe first stage of any episodic memory system is the Ingestion Pipeline, where unstructured text is transformed into structured data. The implementation plan specifies Stanza for this role. This section critically analyzes Stanza's suitability and contrasts it with modern, high-performance alternatives.3.1 Stanza: Linguistic Precision vs. Operational EfficiencyStanza, developed by the Stanford NLP Group, is a Python library that provides a fully neural pipeline for text analysis. It creates a Pipeline object containing various Processors (e.g., Tokenizer, POS Tagger, Dependency Parser) backed by PyTorch models.163.1.1 The OpenIE BottleneckThe specific requirement for an Episodic Memory System is Open Information Extraction (OpenIE)—the ability to extract triples (Subject, Relation, Object) from text without a pre-defined schema (e.g., extracting (User, likes, Sushi)).A critical technical audit of Stanza reveals that its OpenIE capability is not native to its neural pipeline. Instead, Stanza's OpenIE processor acts as a client wrapper around the Stanford CoreNLP Java server.5Architecture: When openie is requested, Stanza launches a local Java process (requiring a JDK installation) and communicates via local sockets.Latency Implications: This architecture introduces significant overhead:Inter-Process Communication (IPC): Serialization and deserialization of data between Python and Java.JVM Startup: The "Cold Start" problem of the Java Virtual Machine can add seconds to the initialization time.18Throughput: Benchmarks indicate that while CoreNLP is robust, it is significantly slower than modern distilled transformers. For example, processing a document can take ~3.87s for deterministic systems versus sub-second latency for optimized neural approaches.19This reliance on Java directly contradicts the "Non-LLM" system's implicit goal of being lightweight and potentially edge-deployable. It introduces a heavy dependency (JDK) that complicates containerization and increases the memory footprint by gigabytes.3.1.2 Coreference Resolution AnalysisEpisodic memory requires resolving pronouns (e.g., mapping "he" in "he went home" to "User"). Stanza 1.7.0 introduced a native coreference model based on the "Conjunction-Aware Word-Level Coreference Resolution" architecture.20Performance: While accurate, this model typically utilizes large Transformer backbones (e.g., RoBERTa-large or Electra-large). Inference on such models is computationally expensive. In a CPU-bound environment (common for "Non-LLM" sidecars), processing a single document can take nearly 5 seconds.19Optimization Gap: The implementation plan does not mention quantization or distillation for this component, suggesting a raw deployment that would choke under high-concurrency conversational loads.3.2 High-Performance Alternatives: GLiNER and FastCorefTo satisfy the requirement for a verified, high-performance implementation, this report identifies superior alternatives that align better with the system's architectural goals.3.2.1 Entity Extraction: GLiNER (Generalist Lightweight Neural Entity Recognition)GLiNER represents a paradigm shift from the pipeline approach of Stanza. It utilizes a bidirectional transformer encoder (BERT-like) to perform zero-shot entity extraction.7Architecture: GLiNER takes a text and a list of arbitrary labels (e.g., ["Person", "Location", "Event"]) and outputs spans. It does not require a fixed schema or retraining.Speed: GLiNER models are compact (~60M to ~300M parameters). On a CPU, GLiNER-small can process text at high speeds, significantly outperforming the heavy Java-based pipeline of CoreNLP.Relation Extraction (GLiREL): Recent extensions (GLiREL) allow for the simultaneous extraction of relations in a single forward pass, eliminating the need for separate dependency parsing and pattern matching.22Verification: GLiNER supports ONNX export and quantization, allowing it to run in the same optimized inference runtime as the embedding models.73.2.2 Coreference: FastCorefFor coreference resolution, FastCoref is the industry standard for efficiency.Methodology: It employs knowledge distillation to train a smaller, faster student model from a larger teacher model (like LingMess).8Benchmark: FastCoref can process ~2.8K documents in 25 seconds on a GPU, whereas comparable models in AllenNLP or Stanza might take minutes.8Latency: On CPU, FastCoref maintains an inference latency that allows for real-time processing of chat streams, satisfying the latency requirements of an interactive agent.3.3 Implementation RecommendationThe verification concludes that Stanza is unsuitable for the high-throughput, low-latency requirements of this system. The implementation plan must be revised to replace Stanza with a GLiNER + FastCoref stack. This change removes the Java dependency, reduces Docker image size, and improves ingestion throughput by an estimated 10x.21FeatureStanza (Proposed)GLiNER + FastCoref (Recommended)LanguagePython + Java (JVM)Pure Python (PyTorch)OpenIE MechanismRule-based over Dependency ParseZero-Shot Neural PredictionCoreference SpeedLow (~1.2s - 5s / doc)High (Sub-second / doc)Deployment WeightHeavy (requires JDK + Models)Light (Distilled Models)ONNX SupportNo (Java components)Yes (Full)4. Technical Verification: The Semantic Representation LayerThe semantic layer is responsible for converting the structured text and graph nodes into vector representations to enable "fuzzy" retrieval (e.g., matching "User likes fruit" when the query is "Does he eat apples?"). The plan specifies Infinity-emb for this role.4.1 Library Analysis: Infinity-embInfinity-emb is a high-throughput embedding server designed for production environments. It is built on top of the sentence-transformers and optimum libraries, providing a FastAPI interface for model serving.104.1.1 Verification of ModernBERT SupportThe user explicitly queried the support for ModernBERT. ModernBERT is a modernized encoder architecture that replaces the absolute positional embeddings of the original BERT with Rotary Positional Embeddings (RoPE) and utilizes Flash Attention, allowing for context windows of up to 8192 tokens.11Compatibility: Infinity-emb supports any model compatible with the Hugging Face transformers library. Since ModernBERT is fully integrated into transformers, Infinity-emb can serve it without modification.10Advantage: This allows the memory system to embed entire "Episodes" (summarized conversations or document chunks) that exceed the 512-token limit of traditional BERT models (like all-MiniLM-L6-v2), preventing the fragmentation of semantic context.4.1.2 Inference Acceleration: ONNX and TensorRTThe report validates that Infinity-emb includes native support for inference acceleration via ONNX Runtime and NVIDIA TensorRT.10Mechanism: When the --engine torch (default) flag is switched to --engine onnx or --engine tensorrt, Infinity-emb compiles the PyTorch computational graph into an optimized intermediate representation. This performs operator fusion (combining multiple mathematical operations into one) and constant folding.Performance Impact: This optimization typically yields a 2x-5x reduction in latency for embedding generation, which is critical when the system must re-embed graph nodes after updates.4.1.3 Dynamic Batching architectureA standout feature of Infinity-emb is its implementation of Dynamic Batching.10Problem: In a live agent system, embedding requests arrive sporadically. Processing them one by one prevents the GPU from saturating its compute cores.Solution: Infinity-emb acts as a queue. It holds incoming requests for a minuscule window (e.g., 10ms) and packs them into a single batch.Result: This allows the system to handle bursts of memory queries (e.g., during a "memory consolidation" phase where the agent reflects on the day's events) with extremely high throughput, without stalling individual queries significantly.4.2 Architectural FitInfinity-emb is verified as an optimal choice. Its support for the latest architectures (ModernBERT) and deployment optimizations (ONNX, Dynamic Batching) makes it superior to using raw sentence-transformers scripts or heavier model servers like Triton (which requires more complex configuration).5. Technical Verification: The Structured Associative Layer (Graphiti)The core differentiation of the proposed system lies in its use of a Knowledge Graph to store episodic data, managed by the Graphiti library. This section analyzes the graph schema, the "Non-LLM" classification, and the storage backend options.5.1 The "Non-LLM" Hybrid ParadoxThe user's request specifies a "Non-LLM Episodic Memory System." A rigorous audit of Graphiti's architecture reveals a potential conflict with this definition.Ingestion Dependency: Graphiti's default "Episode Processing Layer" utilizes LLMs to parse input text into graph edges. The documentation explicitly states it "uses LLMs to identify entities, relationships, and facts".9Cost & Latency: This means that for every piece of memory added, the system makes calls to a generative model (e.g., GPT-4 or Claude). This incurs cost and latency, violating a strict "Non-LLM" constraint if that constraint applies to the entire pipeline.Remediation: However, Graphiti's architecture is modular. The retrieval layer ("hybrid search") does not require an LLM.4 To make the ingestion layer Non-LLM, the implementation plan must override the default extractor with the GLiNER/GLiREL stack recommended in Section 3. This creates a fully deterministic, local ingestion pipeline that feeds into the Graphiti graph structure.5.2 Temporal Modeling: The Power of BitemporalityThe most significant validation of Graphiti is its implementation of Bitemporality. Standard Knowledge Graphs model facts as static triples (Subject, Predicate, Object). Episodic memory requires modelling change.Graphiti treats edges as hyperedges with two time intervals 3:Valid Time ($t_{valid}$): The time period during which the fact is true in the real world.Transaction Time ($t_{tx}$): The time period during which the system believed the fact to be true.5.2.1 Scenario Analysis: The Retroactive CorrectionConsider a user interaction:T1 (Jan 1): User says, "I love steak."System records: (User)-->(Steak).Valid:->(Steak). Valid:->(Vegetarian). `Valid:, is essential for maintaining a coherent narrative in long-term episodic memory, fulfilling the "Manipulability" requirement of the memory system.5.3 Storage Backend: FalkorDB vs. Neo4j vs. SQLiteThe implementation plan must choose a backend database for Graphiti.Neo4j: The default enterprise choice. It is robust but heavy (Java-based).FalkorDB: A Redis-module based graph database. It uses sparse adjacency matrices (Linear Algebra) to perform graph traversals.25Performance: FalkorDB is significantly faster for the type of shallow, dense traversals common in agent memory (e.g., "Find all neighbors of Node X") due to its in-memory architecture and algebraic operations.Vector Support: FalkorDB supports vector indexing natively, allowing for the hybrid search (Vector + Graph) required by Graphiti.26FalkorDBLite: For the "Non-LLM" / Edge use case, FalkorDBLite is the recommended choice. It wraps the FalkorDB engine in a Python library (similar to SQLite), removing the need for external containers or services.27 This aligns perfectly with the goal of a lightweight, self-contained memory system.SQLite: While Graphiti can conceptually run on SQL, traversing graph relationships via SQL JOIN operations is computationally expensive ($O(log n)$ per join) compared to the index-free adjacency traversals ($O(1)$) of a native graph engine. Therefore, SQLite is discouraged unless the graph size is trivial (<1000 nodes).6. Benchmark Auditing and Performance RealityA critical component of this verification is the detection of "overselling"—the tendency of new tools to claim they have "solved" memory based on narrow benchmarks.6.1 Deep Memory Retrieval (DMR)Claim: Zep (Graphiti) achieves 94.8% accuracy on the DMR benchmark, surpassing MemGPT's 93.4%.12Critique:Marginality: The difference of 1.4% is statistically marginal. In a real-world conversation with thousands of turns, this difference is imperceptible to the user.Task Scope: DMR tests Retrieval Precision (finding a specific needle in a haystack). It does not test Reasoning or Synthesis. A system can have 100% DMR score and still fail to answer "Why is the user sad?" if it cannot connect the retrieved facts (e.g., "User lost job" + "User lives in expensive city").Conclusion: While the high score validates the retrieval engine, it does not guarantee a "smarter" agent. It merely guarantees a "better librarian."6.2 LoCoMo (Long-Context Memory)Claim: Graphiti excels in "complex temporal reasoning tasks" and outperforms baselines on benchmarks like LoCoMo.12Critique:Benchmark Difficulty: LoCoMo is an extremely rigorous benchmark involving 300-turn dialogues and tasks like Event Summarization.13The Reality: Research indicates that even the most advanced models (GPT-4-Turbo) lag significantly behind human performance on LoCoMo, specifically in temporal reasoning (Human: ~90%, Model: ~50-60%).14Overselling Warning: If the implementation plan claims that using Graphiti will achieve human-level performance on long-term memory, it is overselling. Graphiti provides the structure to improve reasoning, but the logic (in the retrieval/synthesis step) is still bound by the capabilities of the underlying model. The graph helps reduce hallucinations by providing ground truth, but it does not "solve" the reasoning gap.6.3 EpBench (Episodic Memory Benchmark)Relevance: This benchmark is the most aligned with the goals of an Episodic Memory System. It specifically tests Chronological Awareness (ordering events) and Memory Type Classification (distinguishing episodic from semantic).28Metric: Kendall's τ (Tau) is used to measure the correctness of the sequence of retrieved events.Recommendation: The implementation plan should explicitly include an evaluation step using EpBench. If the Bitemporal Graph is working correctly, the system should score significantly higher on Kendall's τ than a standard Vector RAG system (which essentially treats memory as a "bag of facts" without time).7. Revised Technical Implementation PlanBased on the verification and critique above, this section outlines the corrected, high-performance architecture for the Non-LLM Episodic Memory System.7.1 Architecture OverviewLayerComponentImplementation DetailStatusIngestionGLiNER + FastCorefPython-native, CPU-optimized extraction. Replaces Stanza.RecommendedSemanticInfinity-embModernBERT (ONNX), Dynamic Batching.VerifiedStorageFalkorDBLiteIn-process, sparse-matrix Graph DB with Vector Index.RecommendedOrchestrationGraphiti (Modified)Custom Extractor class using GLiNER; Bitemporal Logic.Verified7.2 Detailed Data FlowInput Processing:The user message enters the system.FastCoref runs on the text to resolve pronouns (e.g., "I went there" -> "User went to Paris").GLiNER extracts entities (User, Paris) and relations (visited).Latency Budget: < 200ms (CPU).Graph Mutation (The Graphiti Logic):The extracted triples are passed to the Graphiti engine.Deduplication: The engine queries FalkorDBLite to see if these nodes exist.Temporal Check: The engine checks for conflicting edges using Bitemporal logic.Update: New edges are written with valid_from=Now. Old conflicting edges are "retired" (valid_to=Now).Latency Budget: < 50ms.Embedding Update:New nodes are sent to Infinity-emb.Vectors are computed (using ModernBERT).Vectors are updated in the FalkorDBLite vector index.Latency Budget: < 100ms (GPU/Accelerated CPU).Retrieval (Hybrid Search):When the agent needs memory, it sends a query.Infinity-emb embeds the query.FalkorDBLite performs a hybrid search:Vector Search finds semantically similar nodes.Graph Traversal expands 1-2 hops from these nodes to find context.Latency Budget: < 50ms.7.3 
8. ConclusionThe "Non-LLM Episodic Memory System" is a viable and necessary evolution in AI architecture. The verification confirms that the conceptual foundation—using a Bitemporal Knowledge Graph to ground agent memory—is sound and addresses the critical failings of context-window-based memory.However, the original implementation plan contained significant technical flaws, primarily the reliance on the heavy, Java-based Stanza library and the implicit use of LLMs within Graphiti's default ingestion pipeline. By adopting the recommendations of this report—specifically the migration to GLiNER/FastCoref and FalkorDBLite—the system can achieve true "Non-LLM" ingestion capability with production-grade latency.While benchmarks like DMR suggest high retrieval performance, the system's ability to reason over long horizons (verified by LoCoMo and EpBench) remains a complex challenge that relies as much on the graph's structure as on the agent's synthesis capabilities. The revised architecture provides the most robust possible foundation for tackling these challenges, offering a memory system that is persistent, manipulable, and deeply grounded in time.9. Appendix: Performance ComparisonsTable 1: Ingestion Pipeline Latency Comparison (Per Document)ComponentTechnologyLatency (CPU)DependencyVerdictStanzaCoreNLP (Java)~3,870 ms 19JDK 11+RejectedStanzaNeural (RoBERTa)~4,960 ms 19PyTorchRejectedFastCorefDistilled Transformer~200 ms 8PyTorchRecommendedGLiNERBi-Encoder~50 ms 29PyTorch/ONNXRecommendedTable 2: Feature Matrix of Graph LibrariesLibraryTemporal ModelVector IndexBackend"Non-LLM" ReadyGraphitiBitemporalHybridNeo4j / FalkorDBYes (with mods)LangChainStaticVector OnlyVariousNoLlamaIndexStaticVector + KeywordVariousNo(Note: Data derived from research snippets 3)

## References
1. **[GLiNER2](https://github.com/urchade/GLiNER)**: Zero-shot/Schema-based entity and relation extraction.
2. **[FastCoref](https://github.com/shon-otmazgin/fastcoref)**: High-performance neural coreference resolution.
3. **[FalkorDBLite](https://github.com/FalkorDB/falkordb-lite)**: Embedded graph database compatible with Redis API.
4. **[Graphiti](https://github.com/getzep/graphiti)**: Conceptual inspiration for bitemporal memory structures.